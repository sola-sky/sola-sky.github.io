---
title: 优化视频解码渲染
date: 2017-02-07 09:05:35
tags: [libyuv ffmpeg]
---

## 问题的出现

在项目中出现 了一个问题就android在使用ffmpeg进行视频的解码渲染的过程中耗时非常的严重，导致的直接现象就是视频播放过程中就像是慢动作回放一样，完全无法满足需求。而反观同事的ios却不会，观其代码，发现其调用ffmpeg的代码跟我这边是差不多的（都是参考网上的代码）。后来即使使用了高配的android手机，发现其效果还是无法满足需求。最后去看开源的ijkplayer，看其是怎么解决的。

## 解决办法

在研究ijkplayer的源码过程中找到了解决方案（只针对FFmpeg、ANativeWindow，并且没有使用OpenGL）：

- 优先采用的是libyuv来进行视频的格式转换的，当无法转换时再采用ffmpeg的`sws_scale()`进行转化。而且发现这种方案在ijkplayer源码中只是针对android平台，而苹果的直接是使用ffmpeg的`sws_sclae()`方法，不需要使用libyuv，至于为什么苹果不需要没有去深究，效率肯定是够了。
- 长宽进行针对不同格式进行对齐操作，不过这块倒是没有去测试过到底快了多少。
- 原先在进行缩放的时候也使用了libyuv对视频进行缩放，将视频长宽缩放到传进来的Surface的长宽，但是实际发现不需要这样做，`ANativeWindow_setBuffersGeomerty()`这个函数会将尺寸自动缩放到窗口实际的物理尺寸。这样就省了libyuv进行缩放操作的时间了
- 还有一个就是进行音视频同步，因为部分帧会解码快、部分比较耗时，通过音视频同步可以控制刷新的快慢或者直接丢帧。
- 通过多线程的方式，一个线程用于解码格式转换，一个线程用于专门的刷新操作，这样的并行操作相比于串行操作来会缩短一些时间

接下来，看看具体的代码：

### 1. 使用libyuv替换ffmpeg的`sws_scale`

```C++
int VoutOverlay::ImageConvert(int width, int height, AVPixelFormat dst_format, uint8_t **dst_data,
                               int *dst_linesize, AVPixelFormat src_format, uint8_t **src_data,
                               const int *src_linesize) {
    if (AV_PIX_FMT_YUV420P == src_format) {
        if (AV_PIX_FMT_RGB565 == dst_format) {
            return libyuv::I420ToRGB565(src_data[0], src_linesize[0],
                                        src_data[1], src_linesize[1],
                                        src_data[2], src_linesize[2],
                                        dst_data[0], dst_linesize[0],
                                        width, height);
        } else if (AV_PIX_FMT_0BGR32) {
            return libyuv::I420ToABGR(src_data[0], src_linesize[0],
                                      src_data[1], src_linesize[1],
                                      src_data[2], src_linesize[2],
                                      dst_data[0], dst_linesize[0],
                                      width, height);
        }
    }
    return -1;
}
```

因为P2P端传过来的视频是YUV420P的格式，这里也只简单判断进行YUV420P转其他格式，这里也是参考ijkplayer写的。这里解释一下libyuv的参数的意思，`I420ToRGB565()`中的第一参数`src_data[0]`代表Y分量，`src_linesize[0]`代表Y分量的长度，剩下的分别代表U、V分量，而最终转化成RGB565时图像数据是放在一个数组进行存储的，所以只需要`dst_data[0]`一个数组就行。详细内容可以上网搜索一下YUV和RGB资料。

### 2. 对长宽进行对齐操作

```C++
VoutOverlay::VoutOverlay(int width, int height, AVPixelFormat frame_format) : width_(width), height_(height), format_(frame_format) {
    opaque_ = new VoutOverlayOpaque();
    pitches_ = opaque_->pitches();
    pixels_ = opaque_->pixels();
  
    ··· ··· 

    if (AV_PIX_FMT_RGB565 == frame_format) {
        buf_width = ALIGN(width, 8);
    } else if (AV_PIX_FMT_RGBA == frame_format) {
        buf_width = ALIGN(width, 4);
    } else {
        buf_width = ALIGN(width, 16)；
    }

    ··· ···
}
```

根据ijkplayer上的提示是**16 bytes align pitch for arm-neon image-convert**，所以最终用于存储转化的视频图像的字节数是16的倍数，因此需要对缓存的宽度进行调整。比如当最终转化为RGB565时，宽度需要是8的倍数，因为RGB565格式是每个像素占用2个字节，所以宽度调整为8的倍数时就能保证最终图像的字节数是16的倍数（n * 8 * 2 == n * 16）。又比如为RGBA时，每个像素占用4字节，所以调整宽度为4的倍数，这样就能确保最终图像是16的倍数（n * 4 * 4 = n * 16）。

### 3. 使用`ANativeWindow_setBuffersGeomerty()`直接进行缩放 

我原来是将视频图像格式YUV420P转化成RGB565后，再次利用libyuv对其进行缩放，以符合Surface的长宽，后面发现ANativeWindow_setBuffersGeometry()本身就有这个功能，不需要用到libyuv。

```C++
/*
 * Change the format and size of the window buffers.
 *
 * The width and height control the number of pixels in the buffers, not the
 * dimensions of the window on screen.  If these are different than the
 * window's physical size, then it buffer will be scaled to match that size
 * when compositing it to the screen.
 *
 * For all of these parameters, if 0 is supplied then the window's base
 * value will come back in force.
 *
 * width and height must be either both zero or both non-zero.
 *
 */
ANativeWindow_setBuffersGeometry(native_window_, buff_w, buff_h, WINDOW_FORMAT_RGB_565);
```

### 4. 音视频同步

音视频同步，其本质是通过比较时间来控制视频的刷新频率，由于某帧导致视频整体慢了，那就缩短后面的视频帧的刷新时间间隔，以此弥补之前的耗时。[音视频同步可以看之前写的一篇文章](http://solasky.info/2017/02/06/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%90%8C%E6%AD%A5%E6%8A%80%E6%9C%AF/)。

### 5. 多线程操作

#### 解码和格式转化线程：

![](http://okxoqauma.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81%E6%B8%B2%E6%9F%93%E2%80%94%E2%80%94%E8%A7%A3%E7%A0%81%E6%A0%BC%E5%BC%8F%E8%BD%AC%E5%8C%96.png)

在一个线程中不断循环执行这一步，不断的获取图片存放于缓存队列中。

具体的代码如下：

```C++
    for (;;) {
        ret = is->GetVideoFrame(ffp, frame); // 从AVPacket队列中取一个AVPacket，然后解析到frame中
        if (ret < 0) {
            av_frame_free(&frame);
            return 0;
        }

        if (!ret) {
            continue;
        }

		··· ··· 
        ret = is->QueuePicture(ffp, frame, pts, duration, av_frame_get_pkt_pos(frame),
                               is->pviddec()->pkt_serial()); // 将frame转化成预期的格式，然后存入图片队列中
        av_frame_unref(frame);
        if (ret < 0) {
            break;
        }
    }
```



#### 视频渲染线程：

![](http://okxoqauma.bkt.clouddn.com/%E4%BC%98%E5%8C%96%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81%E6%B8%B2%E6%9F%93%E2%80%94%E2%80%94%E6%B8%B2%E6%9F%93%E9%83%A8%E5%88%86.png)

视频渲染线程就是不断从图片缓存队列中提取数据，然后根据调整好的时间间隔进行渲染。

具体代码如下：

```C++
void* FFPlayer::VideoRefreshThread(void *arg) {
    assert(arg);
    FFPlayer *player = (FFPlayer *) arg;
    VideoState *is = player->is();
    double remaining_time = 0.0;
    while (!is->abort_request()) {
        if (remaining_time > 0.0) {
            av_usleep((int)(int64_t)(remaining_time * 1000000.0)); // 根据调整的时间间隔睡眠一段时间
        }
        remaining_time = 0.01;
        if (is->show_mode() == SHOW_MODE_VIDEO && (!is->paused() || is->force_refresh())) {
            is->VideoRefresh(player, &remaining_time);  // VideoRefresh()函数会做出一些操作，包括计算延迟，是否丢帧以及是否刷新等。
        }
    }
    return NULL;
}
```

`VidoeRefresh()`函数的相关可以查看之前的一篇文章——[音视频同步技术](http://solasky.info/2017/02/06/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%90%8C%E6%AD%A5%E6%8A%80%E6%9C%AF/)。

## 补充

性能优化这块还可以采用硬编码的形式，不过目前没有去研究；还有可以通过OpenGL来实现，我看ijkplayer上就优先有这种方案。后期可以继续学习ijkplayer的源码，还可以去看看vlc的源码。

